[
  {
    "objectID": "proposal/index.html",
    "href": "proposal/index.html",
    "title": "SMART-MRP",
    "section": "",
    "text": "Background / summary\n(400 words)\nSummary\nSMART-MRP will generate a collection of datasets estimating neighbourhood-level opinion and outcomes from social surveys and non-representative ‘smart data’ – passively-collected observations describing individual-level behaviours and transactions. It will do so by applying Multilevel Regression and Poststratification (MRP), a technique for small area estimation (SAE) little-used outside of Political Polling, but with obvious potential for smart data research. As well as a collection of datasets describing the financial, health and mobility outcomes of populations living in UK neighbourhoods, SMART-MRP will contribute a methodological framework, with code repositories and open source packages, that lower the barrier to entry to MRP in social data analysis. The project will also provide a set of data stories that explain the MRP approach, its underlying uncertainties, and that could be used to assist policy-makers in mobility, health, and sustainability to use the derived datasets for decision-making.\nContext\nIn social research, samples from surveys are routinely used to estimate the characteristics of a population – levels of financial distress, dietary and physical activity, pro-environmental attitudes. With enough sample, we can directly estimate the prevalence of some outcome, behaviour or stated opinion at subnational (regional) level. If we want to look at smaller-scale geographies – in the UK, the 32.8k neighbourhood LSOAs which tend to really discriminate social outcomes – direct estimation soon fails and we must turn to model-based approaches, or small area estimation (SAE).\nIn Multilevel Regression and Poststratification (MRP), the outcome of interest is first modelled (MR) from the survey / microdata using a range of individual (respondent) demographics and group (area-level) context variables. Next, a ‘poststratification frame’ is constructed, whereby, for every small-area unit the analyst wishes to estimate over, joint counts are derived from Census or other administrative data for the different demographics and area-level variables used in the model. At poststratification (P), predicted probabilities of the outcome are extracted from the model for each row of the postratification frame. Multiplying these probabilities by the joint counts, we can estimate the extent of the target outcome occurring in the small-area.\nThe key advantage of MRP is in its use of Bayesian multilevel models. It is common in SAE that certain demographic combinations have little or no representation in the sample data, especially when some additional geographic constraint is added. In these cases, the model uses information from related groups to make more accurate estimates, known as partial pooling. In a famous example, @gelman_forecasting_2016 used MRP on a succession of highly unrepresentative polls of Xbox customers, and documented hoq Xbox-derived forecasts of 2012 US presidential were in-line with those of leading pollsters using large-scale and traditionally sampled surveys. A second advantage of the Bayesian approach is that full posterior distributions are generated for each prediction, and so we can report interpretable indicators of the uncertainty around estimates per spatial unit.\n\n\nAims and objectives\n(300 words)\n\nOutline how the proposal will support delivery of SDR UK’s objectives, including alignment to programme priorities.\n\nThis project will generate a set of place-based data products that simulate public opinion and other outcomes using observational and survey data. We will leverage existing survey datasets, such as the National Travel Survey (NTS), Understanding Society, Food and You 2, as well as observational datasets housed at SDRUK data services – e.g…\nThe project closely overlaps SDR UK priorities insofar as it:\n\nSolves a core methodological challenges: MRP is a principled way of correcting for bias in non-representative samples. Bringing MRP outside of the political polling use case, and applying to behavioural rather than sampled survey data, is a novel contribution.\nBuilds research capability: By producing reproducible code, open-source packages, and explanatory data stories, the project will lower the barrier to entry for MRP implementation, and will emphasise/exploit the intuition behind Bayesian methods.\nGenerates high-impact data products that enhance existing data assets: The datasets describing neighbourhood-level financial, health, and mobility outcomes, with full uncertainty estimates, closely map to SDR UK’s thematic pillars. They will provide a valuable resource for researchers and policy-makers alike, enabling them to address significant social and economic challenges with direct policy relevance.\nPromotes trustworthy and responsible research: Through consistently quantifying and communicating uncertainty information, and borrowing from novel empirically-validated techniques in uncertainty communication. The planned data stories will make these complex outputs accessible, helping to safeguard public trust by transparently explaining the methods and their inherent uncertainties to the target policy-makers and social data analysts who will use them.\n\n\n\nExpected outputs and deliverables\n(300 words)\n\nOutput 1: The data products: TODO: Some detail on how these will be presented/administered – specific datasets, scale of SAEs, uncertainty ranges..\nOutput 2: Reproducible and transferable code products for MRP implementation and validation. TODO: As well as generalisable code, for each data product, we will document the models in full, decisions made around variable inclusion etc. etc..\nOutput 3: Implemented data stories that explain SAEs generated from the MRP, and to assist policy-makers in mobility, health, and sustainability to use the derived datasets for decision-making. TODO: How to express this?\n\n\n\nExpected impact\n(200 words)\nTo discuss, but impact on:\n\nResearch commmunity: the data products, but also presenting MRP, and lowering the barrier of entry to its implementation, for geographers, public health researchers.\nPolicy / practice: Data resource for government and third sector organisations designing interventions in social, public health and mobility. etc. s policy-makers.\nSDR UK: Directly addresses challenges identified in SDR UK strategy – a principled approach to addressing representativeness in smart behavioural data.\n\n\n\nTotal cost and justification of resources\n(300 words)\nTODO.\n\n\nDependencies\n(200 words)\n\ne.g. data availability, acquisition, approval points, resources etc.\n\nTODO.\n\n\nRisks\n(200 words)\n\nMain risks and how they will be mitigated.\n\nTODO.\n\n\nEthics\n(200 words)\nHow any potential ethical and legal issues have been considered and will be addressed.\nTODO."
  },
  {
    "objectID": "demo/index.html",
    "href": "demo/index.html",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "",
    "text": "This document describes a workflow for generating small area outcome estimates using MRP. Originally this work formed a GISRUK abstract (paper | talk). Currently, we are turning this into a full paper, but particularly exciting are the wider ambitions to generate a set of place-based indicators as part of Leeds HASP data service."
  },
  {
    "objectID": "demo/index.html#introduction",
    "href": "demo/index.html#introduction",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "",
    "text": "This document describes a workflow for generating small area outcome estimates using MRP. Originally this work formed a GISRUK abstract (paper | talk). Currently, we are turning this into a full paper, but particularly exciting are the wider ambitions to generate a set of place-based indicators as part of Leeds HASP data service."
  },
  {
    "objectID": "demo/index.html#setup",
    "href": "demo/index.html#setup",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "Setup",
    "text": "Setup\nAs well as the usual suspects, listed below are some useful packages for working over (Bayesian) models in a tidy way. brms is a wrapper to Stan; marginaleffects is a very useful package for generating interpretable outputs from model objects (conditional effects in this case); and tidybayes for uncertainty representation extensions to ggplot2.\n\n\nCode\nlibrary(here) \nlibrary(tidyverse) \n\n# Bayesian modelling tools\nlibrary(rstanarm)\nlibrary(cmdstanr)\nlibrary(brms)\nlibrary(marginaleffects) # For extracting effect sizes from bayesian objects.\nlibrary(parameters)\nlibrary(tidybayes) # For uncertainty representation.\n\nlibrary(sf)\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(ggpubr)\n\n# library(modelsummary)\n# library(gt)\nsource(here(\"R\", \"theme_clean.R\"))\nsource(here(\"R\", \"plot_helpers.R\"))"
  },
  {
    "objectID": "demo/index.html#data",
    "href": "demo/index.html#data",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "Data",
    "text": "Data\n\nSurvey data (microdata)\nCensus data: Pop by usual residence in households.\nPoststratification frame: Joint counts (and modelled joint counts) by MSOA.\nMSOA lookup files, spatial geometry files etc.\n\n\nHealth Survey for England data\n\n\nCode\n# HEStoFMF.R &gt; Converts to format to be used in FMF\n# HES data (microdata)\nhes_raw &lt;- read_csv(here(\"Data\", \"HES\", \"spss\", \"spss28\", \"HES_for_R.csv\"))\nhes_for_model &lt;- read_csv(here(\"FMF\", \"HES\", \"HESforFMF.csv\")) |&gt; \n  select(id=Seriala, health=genhelf1, SexbyAge, sha=SHA2, region=GOR2, is_urban=urban14b_2, imd=qimd19_2) |&gt;\n  separate_wider_delim(col=SexbyAge, names=c(\"sex\", \"age\"), delim=\".\") |&gt; \n  mutate(\n    # Create binary outcome variable.\n    is_good=health %in% c(\"good\",\"verygood\"),\n    # Cast 5-point as factor, for ordinal regression.\n    health=factor(health, levels=c(\"verybad\", \"bad\", \"fair\", \"good\", \"verygood\"), ordered=TRUE),\n    is_urban=is_urban==\"U\",\n    imd=case_when(\n      imd==\"Least.deprived\" ~ \"5\",\n      imd== \"X4\" ~ \"2\", \n      imd== \"X3\" ~ \"3\",\n      imd== \"X2\" ~ \"4\",\n      imd==\"Most.deprived\" ~ \"1\",\n      TRUE ~ imd\n      ),\n    across(c(sha, region), ~str_replace_all(.x, \"\\\\.\",\"_\"))\n    ) |&gt; \n    left_join(hes_raw |&gt;  mutate(educ= case_when(\n    topqual3 == \"NVQ4/NVQ5/Degree or equiv\" ~ \"level4\",\n    topqual3 %in% c(\"Higher ed below degree\",\"NVQ3/GCE A Level equiv\") ~ \"level3\",\n    topqual3 %in% c(\"NVQ1/CSE other grade equiv\",\"NVQ2/GCE O Level equiv\") ~ \"level1level2\",\n    topqual3 == \"Foreign/other\" ~ \"other\",\n    topqual3 %in% c(\"No qualification\",\"Refused\") ~ \"level0\",\n    topqual3 == \"Not applicable\" ~ \"na\",\n    TRUE ~ topqual3\n  )) |&gt; \n  select(id=Seriala, educ)) |&gt; \n    mutate(age2=case_when(\n      age %in% c(\"age0to4\", \"age5to15\") ~ \"age0to15\",\n      age %in% c(\"age65to74\", \"age75p\") ~ \"age65p\",\n      TRUE ~ age\n    )) \n\n\n\n\nCensus data and poststratification frame\n\n\nCode\n# Total households by MSOA\nzone_totals &lt;- read_csv(here(\"data\", \"TS001.csv\")) |&gt; rename(zone_id=`Zone ID`)\n\n# Groundtruth: Census data\nhealth &lt;- read_csv(here(\"data\", \"GH.csv\")) |&gt; rename(zone_id=`Zone ID`)\ngeog &lt;- readxl::read_excel(here(\"data\", \"MSOA21_GOR_SHA_UR_IMD.xlsx\"), sheet=\"MSOA21\") |&gt; \n  select(zone_id=MSOA21CD, region=GOR2, sha=SHA2, is_urban=urban14b_2, imd=IMD19_1eqMD) |&gt; \n  mutate(\n  is_urban=is_urban==\"U\", across(c(sha, region), ~str_replace_all(.x, \"\\\\.\",\"_\")))\n\n# Poststratification frame\n# ps1 : sex by age\nps1 &lt;- read_csv(here(\"data\", \"SexbyAge.csv\")) |&gt; rename(zone_id=`Zone ID`) |&gt; \n  pivot_longer(-zone_id) |&gt; \n  separate_wider_delim(col=name, names=c(\"sex\", \"age\"), delim=\".\") |&gt; \n  mutate(count=value) |&gt; \n  group_by(zone_id) |&gt; \n  mutate(zone_tot=sum(count)) |&gt; ungroup() |&gt; \n  group_by(zone_id, sex, age) |&gt;\n  summarise(cell_counts=sum(count), prop=cell_counts/zone_tot) |&gt; \n  ungroup() |&gt; \n  left_join(geog)\n\n# ps2 : sex by age by education level\nps2 &lt;- readxl::read_excel(here(\"data\", \"SbyAbyQual.xlsx\"), sheet=\"custom-filtered-2024-11-24T15_1\")\nnames(ps2) &lt;- c(\"zone_id\", \"zone_name\", \"age_code\", \"age\", \"sex_code\", \"sex\", \"educ_code\", \"educ\", \"count\")\n\n# Checking for different MSOA definitions.\nmissing_msoas &lt;- ps2 |&gt; \n  separate_wider_delim(educ, delim=\":\", names=c(\"educ\", \"expl\"), too_few = \"align_start\") |&gt; \n  select(zone_id, age, sex, educ, count) |&gt; \n  left_join(geog |&gt; select(zone_id, region)) |&gt; filter(region!=\"Wales\") |&gt; \n  group_by(zone_id) |&gt;\n  summarise(zone_tot=sum(count)) |&gt; \n  left_join(geog) |&gt; filter(region!=\"Wales\") |&gt; select(zone_id, region) |&gt; unique() |&gt; \n  left_join(ps1 |&gt; select(zone_id, r2=region) |&gt; unique() ) |&gt; filter(is.na(r2)) |&gt; pull(zone_id)\n\n# Create formatted ps dataset, matching HSE data.\nps2  &lt;- ps2 |&gt; \n  separate_wider_delim(educ, delim=\":\", names=c(\"educ\", \"expl\"), too_few = \"align_start\") |&gt; \n  select(zone_id, age, sex, educ, count) |&gt; \n  left_join(geog |&gt; select(zone_id, region)) |&gt; filter(region!=\"Wales\", !zone_id %in% missing_msoas) |&gt; \n  select(-region) |&gt; \n  mutate(\n    educ= case_when(\n      educ == \"Level 4 qualifications or above\" ~ \"level4\",\n      educ == \"Level 3 qualifications\" ~ \"level3\",\n      educ %in% c(\"Level 2 qualifications\",\"Level 1 and entry level qualifications\") ~ \"level1level2\",\n      educ == \"No qualifications\" ~ \"level0\",\n      educ == \"Does not apply\" ~ \"na\",\n      educ == \"Other\" ~ \"other\",\n      TRUE ~ as.character(educ) ),\n    sex=tolower(sex),\n    age = case_when(\n      age == \"Aged 15 years and under\" ~ \"age0to15\",\n      age == \"Aged 16 to 24 years\" ~ \"age16to24\",\n      age == \"Aged 25 to 34 years\" ~ \"age25to34\",\n      age == \"Aged 35 to 49 years\" ~ \"age35to49\",\n      age == \"Aged 50 to 64 years\" ~ \"age50to64\",\n      age == \"Aged 65 years and over\" ~ \"age65p\",\n      TRUE ~ age ),\n    ) |&gt;\n  group_by(zone_id) |&gt;\n  mutate(zone_tot=sum(count)) |&gt; ungroup() |&gt; \n  group_by(zone_id, sex, age, educ) |&gt;\n  summarise(cell_counts=sum(count), prop=cell_counts/zone_tot) |&gt;   \n  ungroup() |&gt; unique() |&gt; \n  left_join(geog)\n\n# Identify differences in MSOA codes between the HSE and MSOA lookup datasets.\n# missing_msoas &lt;- t |&gt; left_join(geog) |&gt; filter(region!=\"Wales\") |&gt; select(zone_id, region) |&gt; unique() |&gt; left_join(ps1|&gt; select(zone_id, r2=region) |&gt; unique() ) |&gt; filter(is.na(r2)) |&gt; pull(zone_id)\n# LIST of MSOAs not in ps1but present in ps2\n# Joining with `by = join_by(zone_id)`\n# Joining with `by = join_by(zone_id)`\n# # A tibble: 14 × 3\n#    zone_id   region                   r2   \n#    &lt;chr&gt;     &lt;chr&gt;                    &lt;chr&gt;\n#  1 E02002524 North_East               NA   \n#  2 E02002526 North_East               NA   \n#  3 E02002693 Yorkshire_and_the_Humber NA   \n#  4 E02003897 South_West               NA   \n#  5 E02003972 North_West               NA   \n#  6 E02003974 North_West               NA   \n#  7 E02004243 South_West               NA   \n#  8 E02004631 South_West               NA   \n#  9 E02005184 North_West               NA   \n# 10 E02005327 North_West               NA   \n# 11 E02005713 North_East               NA   \n# 12 E02005721 North_East               NA   \n# 13 E02005727 North_East               NA   \n# 14 E02005915 East_Midlands            NA   \n\n# Population size of those missing MSOAs : 88536\nps2 |&gt; left_join(geog) |&gt; \n  filter(region!=\"Wales\", !zone_id %in% missing_msoas ) |&gt; \n  summarise(tot=sum(cell_counts))\n# 55415266\nps1|&gt; summarise(tot=sum(cell_counts))\n# 55415499\n# Overall difference in zone totals, with hhd also of 233.\n\n# Check levels exactly match between the survey and postratification frame.\nall.equal(\n  ps2 |&gt; select(region) |&gt; unique() |&gt; arrange(region) |&gt;  pull(),\n  hes_for_model |&gt; select(region) |&gt; unique() |&gt; arrange(region) |&gt; pull()\n)\n\n\n\n\nBoundary data\n\n\nCode\n# MSOAs : Bring down from ONS Open Geography Portal and simplify. \n# msoa_boundaries &lt;- st_read(\"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/Middle_layer_Super_Output_Areas_December_2021_Boundaries_EW_BSC_V3/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\") |&gt; \n# rmapshaper::ms_simplify(keep_shapes = TRUE)\n# st_write(msoa_boundaries, here(\"data\", \"msoa_boundaries.geojson\"))\nmsoa_boundaries &lt;- st_read(here(\"data\", \"msoa_boundaries.geojson\"))"
  },
  {
    "objectID": "demo/index.html#explore-survey-data-look-for-bias",
    "href": "demo/index.html#explore-survey-data-look-for-bias",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "Explore survey data: Look for bias",
    "text": "Explore survey data: Look for bias\n\n\nCode\n# Binary outcome.\ntemp_freqs &lt;- bind_rows (\n  ps2 |&gt; \n  select(c(sex:cell_counts, region, is_urban, imd)) |&gt; mutate(is_urban=as.character(is_urban), imd=as.character(imd)) |&gt; \n  pivot_longer(-c(cell_counts), names_to = \"var_name\", values_to = \"val\") |&gt; \n  group_by(var_name, val) |&gt; \n  summarise(count=sum(cell_counts)) |&gt; mutate(type=\"Census\"),\n  \n  hes_for_model |&gt; select(-c(is_good,sha, age)) |&gt; \n  rename(age=age2) |&gt; \n  mutate(is_urban=as.character(is_urban)) |&gt; \n  pivot_longer(-c(id, health), names_to = \"var_name\", values_to = \"val\") |&gt; \n  group_by(var_name, val) |&gt; \n  summarise(count=n()) |&gt;  mutate(type=\"HSE\")\n)\n\n# Plot\nbias_hse &lt;- temp_freqs |&gt; \n  pivot_wider(names_from=var_name, values_from=val) |&gt; \n  mutate(\n    imd=case_when(\n      imd==\"1\" ~ \"1 most\",\n      imd== \"5\" ~ \"5 least\",\n      TRUE ~ imd\n      ),\n    region=case_when(\n      region==\"East_Midlands\" ~ \"EM\",\n      region==\"East_of_England\" ~ \"E\", \n      region==\"North_East\" ~ \"NE\",\n      region==\"North_West\" ~ \"NW\",\n      region==\"South_East\" ~ \"SE\",\n      region==\"South_West\" ~ \"SW\",\n      region==\"West_Midlands\" ~ \"WM\",\n      region==\"Yorkshire_and_the_Humber\" ~ \"Y&H\",\n      TRUE ~ region),\n    educ=case_when(\n      educ==\"level0\" ~ \"0. none\",\n      educ==\"level1level2\" ~ \"1. 2+ GCSEs\", \n      educ==\"level3\" ~ \"3. A-levels\", \n      educ==\"level4\" ~ \"4. degree +\",\n      educ==\"na\" ~ \"na/other\",\n       educ==\"other\" ~ \"na/other\",\n      ),\n    age=case_when(\n      age == \"age0to15\" ~ \"0-15\",\n      age == \"age16to24\" ~ \"16-24\",\n      age == \"age25to34\" ~ \"25-34\",\n      age == \"age35to49\" ~ \"35-49\",\n      age == \"age50to64\" ~ \"50-64\",\n      age == \"age65p\" ~ \"65+\"),\n    is_urban=case_when(\n      is_urban == FALSE ~ \"rural\",\n      is_urban == TRUE ~ \"urban\")\n  ) |&gt; \n  pivot_longer(-c(count, type), names_to = \"var_name\", values_to = \"val\") |&gt; filter(!is.na(val)) |&gt; \n  mutate(var_name=factor(var_name, levels=c(\"sex\", \"age\", \"educ\", \"imd\", \"is_urban\", \"region\"))) |&gt; \n  \n  group_by(type, var_name) |&gt; \n  mutate(tot=sum(count)) |&gt; ungroup() |&gt; \n  group_by(type, val) |&gt; \n  mutate(prop=count/tot) |&gt; ungroup() |&gt; \n  \n  ggplot(aes(x=val, y=prop)) +\n  geom_col(data=. %&gt;% filter(type==\"HSE\"), fill=\"#636363\", alpha=.5) +\n  geom_spoke(data=. %&gt;% filter(type==\"Census\"), angle=0, radius=.5, position=\"center_spoke\",alpha=1, linewidth=.8, colour=\"#08519c\") +\n  facet_grid(.~var_name, scales=\"free_x\", space=\"free_x\") +\n  labs(\n    y=\"% in &lt;span style='color: #08519c;'&gt;**Census**&lt;/span&gt; vs &lt;span style='color: #636363;'&gt;**HSE**&lt;/span&gt;\", \n    x=\"\", \n    title=\"Difference between 2021 &lt;span style='color: #636363;'&gt;**HSE**&lt;/span&gt; sample and &lt;span style='color: #08519c;'&gt;**Census**&lt;/span&gt; population\") +\n  theme(\n    strip.text.x = element_text(size=35),\n    axis.text.y=element_text(size=30),\n    axis.text.x=element_text(angle=270, hjust = 0, size=35),\n    axis.title.y = element_markdown(size=40),\n    plot.title = element_markdown(size=60)\n    )\nggsave(here(\"..\", \"figs\", \"bias_hse2.png\"), bias_hse, width=8, height=4.5, dpi=300)\n\n\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "demo/index.html#model",
    "href": "demo/index.html#model",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "Model",
    "text": "Model\nI am following primers on MRP:\n\nJuan Lopez-Martin + others, via brms and stanarm\nSolomon Kurz, via brms\nNathan Alexander, via stanarm\n\nAlso, I have been consulting Andrew Heiss’s detailed posts on working with logistic regression multilevel model outputs: here and here.\nGeneral practice in MRP is to include every variable as a random intercept / level and so fully-pool estimates of the outcome. That makes sense, but it means that it’s hard to interpret and justify model-based decisions on variables included in the model. While general practice is to include as many demographics and geography variables as possible, as long as they can still compute, if we are interested in ultimately building some SPM, then it is useful to require parsimony in the variables used. But interpreting these hierarchical Bayesian model outputs is hard. What I do here is:\n\nGenerate posterior predictions for each level in the model and look at how narrow the CIs are for those values. This is using marginal effects.\nGenerate posterior conditional effects for each level in them, again using marginal effects.\nExtract the “condition_mean”, as per Alexander 2023 – but I need to check what this means.\n\n\nPriors\nWe parameterise our models using:\n\nAn informed prior on the intercept. We have some experience with stated response surveys and health outcomes. Our expectation is that most will identify as in good health – c. 75%. So for the multilevel logistic regression, our intercept is a Gaussian set on log-odds scale, we use \\(Normal(mean=0.5, sd=1)\\).\nThe multilevel priors are set to \\(exponential(1)\\) as a default.\nThe adept_delta setting is to .95 – common when a hierarchical grouping variable has few levels.\n\n\n\nCode\np &lt;- tibble(n = rnorm(1e6, mean = 0.5, sd = 1)) |&gt;  \n  mutate(p = brms::inv_logit_scaled(n)) |&gt; \n  ggplot(aes(x = p)) +\n  geom_histogram(fill=\"#636363\", binwidth = .02, boundary = 0, alpha=.5) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(\n    title=\"&lt;span&gt;theoretical **prior** on intercept&lt;/span&gt;\", \n    x=\"baseline % **good** health\", y=\"\") +\n  theme(\n    axis.title=element_markdown(size=35), \n    plot.title = element_markdown(size=60),\n    axis.text.x=element_text(size=30)\n    )\nggsave(here(\"..\", \"figs\", \"prior.png\"), p, width=4.5, height=3, dpi=300)\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nExample model specification\n\nset.seed(13)\n# Time difference of 5.53914 mins\nstart &lt;- Sys.time()\nmodel7 &lt;-\n  brm(data = hes_for_model,\n      family = bernoulli(),\n      is_good ~ sex + (1 | age2) + (1 | educ) + (1 | imd) + (1 | region) + (1 | is_urban), #+ (1 | educ:age2),\n      #is_good ~ sex + (1 | age),\n      prior = c(set_prior(\"normal(0.5, 1)\", class = \"Intercept\"),\n                set_prior(\"normal(0, 1)\", class = \"b\"),\n                set_prior(\"exponential(1)\", class = \"sd\", group=\"age2\"),\n                set_prior(\"exponential(1)\", class = \"sd\", group=\"educ\"),\n                set_prior(\"exponential(1)\", class = \"sd\", group=\"imd\"),\n                set_prior(\"exponential(1)\", class = \"sd\", group=\"region\"),\n                set_prior(\"exponential(1)\", class = \"sd\", group=\"is_urban\")#,\n                # set_prior(\"exponential(1)\", class = \"sd\", group=\"educ:age2\")\n                ),\n      iter = 2000, warmup = 1000, cores = 4, chains=4,\n      seed = 13,\n      backend = \"cmdstanr\", \n      control = list(adapt_delta = .95))\nprint(Sys.time()-start)\n# saveRDS(model, file = here(\"MRP\", \"fit_hse_7.rds\"))\n# saveRDS(model, file = here(\"MRP\", \"fit_hse_7_nointeract.rds\"))\n# model &lt;- readRDS(here(\"..\", \"models\", \"fit_hse_7.rds\"))\nmodel7 &lt;- readRDS(here(\"..\", \"models\" \"fit_hse_7_nointeract.rds\"))\n\nmodel1 &lt;-\n  brm(data = hes_for_model,\n      family = bernoulli(),\n      is_good ~ sex + (1 | age2),\n      prior = c(set_prior(\"normal(0.5, 1)\", class = \"Intercept\"),\n                set_prior(\"normal(0, 1)\", class = \"b\"),\n                set_prior(\"exponential(1)\", class = \"sd\", group=\"age2\")\n                ),\n      iter = 2000, warmup = 1000, cores = 4, chains=4,\n      seed = 13,\n      backend = \"cmdstanr\", \n      control = list(adapt_delta = .95))\n\n# saveRDS(model1, file = here(\"..\", \"models\", \"fit_hse_1.rds\"))\nmodel1 &lt;- readRDS(here(\"..\", \"models\", \"fit_hse_1.rds\"))\n\n\n\nExample analysis of model outputs\nThe benefit of modern packages for computational model-building (and Bayesian approach) is that we can take advantage of resampling procedures to derive more interpretable quantities for the variables included in our model. For example, we can ask about the predicted probability of a particular characteristic individual, or an individual living in a particular place, reporting a good health outcome. Or what the average difference in likelihood of reporting a good health outcome is for a hypothetical individual of a particular age versus a hypothetical individual of another age group (conditional on other characteristics being consistent with the sample average).\nThe marginaleffects package provides a set of functions to do this – to look at posterior predicted values and comparisons (contrasts) for different, user specified, individuals. There are a few key functions:\n\nmarginaleffects::predictions() extracts from a model the predicted probability in this case of a survey respondent reporting a good health outcome. To this function we supply a newdata argument with a grid of person characteristics that we want to predict over/for, and a type argument setting the scale with which to return predictions. By default marginaleffects::predictions() returns for each row (respondent) in the dataset, but we might want to return predicted values for:\n\nA specific demographic combination (as in postratification frame)\nA hypothetical individual with each predictor held at the sample average (mean / mode).\n\nmarginaleffects::avg_predictions() takes the average of all predicted values of the dataset – so imagine predicting out for each observation (respondent) and averaging out their predicted probabilities. But this becomes more useful when we pass a by argument to predict for actually observed values of our dataset – e.g. for particular ages or other characteristics, what is the average probability of the outcome, given our model?\nmarginaleffects::comparisons() compares two sets of predictions made with different predictor values. The quantity of interest is the difference in predictions when, say, age takes on different values. This is done via parameterising the variables argument. Risk differences are conditional quantities, which vary with the values of all predictors in the model. By default we obtain the comparison (risk difference) for each unit of observation.\n\nSo below, I inspect the predicted probability of good health by key demographics and comparing model1 (using only sex and age) with model7, which uses many more covariates.\n\n\nCode\nvar_levels &lt;- tibble(variable = c(\"age2\", \"educ\", \"region\", \"imd\", \"is_urban\")) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- hes_for_model[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else if (is.logical(x)) {\n        levels(factor(x))\n    }\n    else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels)  \n\nmodel_var_levels &lt;- bind_rows(\n  var_levels |&gt; filter(variable == \"age2\") |&gt; add_column(model=\"model1\"), \n  var_levels |&gt; add_column(model=\"model7\")\n)\n\n\nmodel_preds &lt;- map2_df(\n  .x = list(model1, model7), \n  .y = c(\"model1\", \"model7\"),\n  ~ map_df(model_var_levels |&gt; filter(model == .y) |&gt; pull(variable) |&gt; unique(),\n           function(var) {\n             avg_predictions(.x, by = var, allow_new_levels = TRUE) |&gt; \n               posterior_draws() |&gt;  \n               add_column(condition = var, model = .y) \n           })\n)\n\nplot_data &lt;- model_preds |&gt;  \n  mutate(age2 = case_when(\n      str_detect(age2, \"age\\\\d+p$\") ~ str_replace(age2, \"age(\\\\d+)p\", \"\\\\1+\"),\n      str_detect(age2, \"age\\\\d+to\\\\d+\") ~ str_replace(age2, \"age(\\\\d+)to(\\\\d+)\", \"\\\\1 - \\\\2\"),\n      TRUE ~ age2),\n    imd=case_when(\n      imd==\"1\" ~ \"1 most\",\n      imd== \"5\" ~ \"5 least\",\n      TRUE ~ imd\n      ),\n    region=case_when(\n      region==\"East_Midlands\" ~ \"EM\",\n      region==\"East_of_England\" ~ \"E\", \n      region==\"North_East\" ~ \"NE\",\n      region==\"North_West\" ~ \"NW\",\n      region==\"South_East\" ~ \"SE\",\n      region==\"South_West\" ~ \"SW\",\n      region==\"West_Midlands\" ~ \"WM\",\n      region==\"Yorkshire_and_the_Humber\" ~ \"Y&H\",\n      TRUE ~ region),\n    educ=case_when(\n      educ==\"level0\" ~ \"0. none\",\n      educ==\"level1level2\" ~ \"1. 2+ GCSEs\", \n      educ==\"level3\" ~ \"3. A-levels\", \n      educ==\"level4\" ~ \"4. degree +\",\n      educ==\"na\" ~ \"na/other\",\n      educ==\"other\" ~ \"na/other\",\n      ),\n    is_urban=case_when(\n      is_urban == FALSE ~ \"rural\",\n      is_urban == TRUE ~ \"urban\"),\n    cat_val = case_when(\n      condition == \"age2\" ~ pick(age2) |&gt;  pull(),\n      condition == \"educ\" ~ pick(educ) |&gt;  pull(),\n      condition == \"region\" ~ pick(region) |&gt;  pull(),\n      condition == \"imd\" ~ pick(imd) |&gt;  pull(),\n      condition == \"is_urban\" ~ pick(is_urban) |&gt;  pull(),\n      TRUE ~ NA_character_\n    )\n  )\n  \np &lt;- plot_data |&gt; \n  filter(!(educ == \"na/other\" & condition == \"educ\")) |&gt; \n  ggplot(aes(x = draw, y = cat_val)) +\n  stat_gradientinterval(\n    slab_size = 3,  \n    #normalize = \"xy\", \n    show_interval = FALSE,\n    show_point = FALSE,   \n    colour=\"#525252\",\n   fill=\"#525252\"\n  )  +\n  \n  geom_spoke(data = . %&gt;% group_by(model, condition, cat_val) %&gt;% summarise(draw=mean(draw)), angle=get_radians(90), radius=.5, position=\"center_spoke\",alpha=1, linewidth=.5, colour=\"#525252\") + \n  \n  scale_x_continuous(labels = scales::label_percent(), breaks=c(.5,.7,.9)) +\n  labs(\n    x = \"Predicted probability of **good** health\", y = NULL,\n    title = \"Predicting over demographics by **model**\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank()) +\n  geom_vline(xintercept=.777, colour=\"#525252\", linewidth=.2) +\n  coord_cartesian(xlim = c(.45, 1)) + \n  facet_grid(condition~model, scales=\"free_y\", space=\"free_y\") +\n  theme(\n    axis.title = element_markdown(size=35),\n    plot.title = element_markdown(size=60),\n    axis.text = element_text(size=30),\n    strip.text = element_text(size=30)\n  )\nggsave(here(\"..\", \"figs\", \"predictions.png\"), p, width=6.5, height=5, dpi=300)\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nHere I use avg_comparisons() to analyse the average marginal component effect of moving between age ranges, education levels etc. holding other features constant. So with marginal effects, we ask what happens to an outcome when the explanatory variable moves, in this case, between levels. This corresponds with how one would normally interpret coefficients from a regression model.\nThis function works as follows, as per marginaleffects documentation:\n\n\nCompute predictions for every row of the dataset in the counterfactual world where all observations belong to the treatment condition.\nCompute predictions for every row of the dataset in the counterfactual world where all observations belong to the control condition.\nTake the differences between the two vectors of predictions.\nAverage the unit-level estimates across the whole dataset, or within subgroups.\n\n\n\n\nCode\n# Time difference of 2.9 mins\nstart &lt;- Sys.time() \nmodel_effects &lt;- map2_df(\n  .x = list(model1, model7), \n  .y = c(\"model1\", \"model7\"),\n  ~ map_df(model_var_levels |&gt; filter(model == .y) |&gt; pull(variable) |&gt; unique(),\n           function(var) {\n             avg_comparisons(.x, variable = var, allow_new_levels = TRUE) |&gt; \n               posterior_draws() |&gt; \n               add_column(condition= var, model = .y)\n           })\n)\nprint(Sys.time()-start)\n\n\neffects_nested &lt;- model_effects %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(variable_level, reference_level, model) %&gt;% \n  nest()\n\n\nmap2_df(\n  .x = list(model1, model7), \n  .y = c(\"model1\", \"model7\"),\n  ~ map_df(model_var_levels |&gt; filter(model == .y) |&gt; pull(variable) |&gt; unique()\n\n\nplot_data &lt;- model_var_levels |&gt; \n  left_join(\n    effects_nested,\n    by = join_by(levels == variable_level, model == model)\n  ) |&gt; \n  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) |&gt; \n  unnest(data) \n\nplot_data_m1 &lt;- plot_data |&gt; filter(model ==\"model1\") |&gt; \n  pivot_wider(names_from=condition, values_from=levels) |&gt; \n  mutate(age2 = case_when(\n      str_detect(age2, \"age\\\\d+p$\") ~ str_replace(age2, \"age(\\\\d+)p\", \"\\\\1+\"),\n      str_detect(age2, \"age\\\\d+to\\\\d+\") ~ str_replace(age2, \"age(\\\\d+)to(\\\\d+)\", \"\\\\1 - \\\\2\"),\n      is.na(age2) ~ \"0 - 15\",\n      TRUE ~ age2),\n    cat_val = case_when(\n      variable == \"age2\" ~ pick(age2) |&gt;  pull(),\n      TRUE ~ NA_character_\n    )\n  ) |&gt; \n  select(model, variable, draw, cat_val)\n  \nplot_data_m7 &lt;- plot_data |&gt; filter(model ==\"model7\") |&gt; \n  pivot_wider(names_from=condition, values_from=levels) |&gt; \n  mutate(age2 = case_when(\n      str_detect(age2, \"age\\\\d+p$\") ~ str_replace(age2, \"age(\\\\d+)p\", \"\\\\1+\"),\n      str_detect(age2, \"age\\\\d+to\\\\d+\") ~ str_replace(age2, \"age(\\\\d+)to(\\\\d+)\", \"\\\\1 - \\\\2\"),\n      is.na(age2) ~ \"0 - 15\",\n      TRUE ~ age2),\n    imd=case_when(\n      imd==\"1\" ~ \"1 most\",\n      imd== \"5\" ~ \"5 least\",\n      is.na(imd) ~ \"1 most\",\n      TRUE ~ imd\n      ),\n    region=case_when(\n      region==\"East_Midlands\" ~ \"EM\",\n      region==\"East_of_England\" ~ \"E\", \n      region==\"North_East\" ~ \"NE\",\n      region==\"North_West\" ~ \"NW\",\n      region==\"South_East\" ~ \"SE\",\n      region==\"South_West\" ~ \"SW\",\n      region==\"West_Midlands\" ~ \"WM\",\n      region==\"Yorkshire_and_the_Humber\" ~ \"Y&H\",\n      is.na(region) ~ \"EM\",\n      TRUE ~ region),\n    educ=case_when(\n      educ==\"level0\" ~ \"0. none\",\n      educ==\"level1level2\" ~ \"1. 2+ GCSEs\", \n      educ==\"level3\" ~ \"3. A-levels\", \n      educ==\"level4\" ~ \"4. degree +\",\n      educ==\"na\" ~ \"na/other\",\n      educ==\"other\" ~ \"na/other\",\n      is.na(educ) ~ \"0. none\",\n      TRUE ~ educ\n      ),\n    is_urban=case_when(\n      is_urban == FALSE ~ \"rural\",\n      is_urban == TRUE ~ \"urban\",\n      is.na(is_urban) ~ \"rural\",\n      TRUE ~ is_urban\n      ),\n    cat_val = case_when(\n      variable == \"age2\" ~ pick(age2) |&gt;  pull(),\n      variable == \"educ\" ~ pick(educ) |&gt;  pull(),\n      variable == \"region\" ~ pick(region) |&gt;  pull(),\n      variable == \"imd\" ~ pick(imd) |&gt;  pull(),\n      variable == \"is_urban\" ~ pick(is_urban) |&gt;  pull(),\n      TRUE ~ NA_character_\n    )\n  ) |&gt; \n  select(model, variable, draw, cat_val)\n\nplot_data &lt;- bind_rows(plot_data_m1, plot_data_m7)  \n\np &lt;- plot_data |&gt; \n ggplot(aes(x = draw, y = cat_val)) +\n  stat_gradientinterval(\n    slab_size = 3,  \n    #normalize = \"xy\", \n    show_interval = FALSE,\n    show_point = FALSE,   \n    colour=\"#525252\",\n   fill=\"#525252\"\n  )  +\n  \n  geom_spoke(data = . %&gt;% group_by(model, variable, cat_val) %&gt;% summarise(draw=mean(draw)), angle=get_radians(90), radius=.5, position=\"center_spoke\",alpha=1, linewidth=.5, colour=\"#525252\") + \n  \n  \n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank()) +\n  geom_vline(xintercept=0, colour=\"#525252\", linewidth=.2) +\n  \n  scale_x_continuous(labels = scales::label_percent()) +\n  \n  facet_grid(variable~model, scales=\"free_y\", space=\"free_y\") +\n  \n  #facet_grid(variable~., scales=\"free_y\", space=\"free_y\") +\n   labs(\n    y = NULL,\n    title = \"Posterior 'marginal effects'\",\n    #subtitle = \"-- moving between levels of explanatory variables\",\n    x=\"% change in **good** health from control level\", y=\"\"\n  ) +\n  theme(\n    axis.title = element_markdown(size=35),\n    plot.title = element_markdown(size=60),\n    plot.subtitle = element_markdown(size=35),\n    axis.text = element_text(size=30),\n    strip.text = element_text(size=30)\n  )\nggsave(here(\"..\", \"figs\",\"comparisons.png\"), p, width=6.5, height=5, dpi=300)\n\n\n\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "demo/index.html#example-poststratification",
    "href": "demo/index.html#example-poststratification",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "Example poststratification",
    "text": "Example poststratification\nAt poststratification, we predict over the outcome for each of the ‘strata’ defined by the poststratification frame (ps). I’ve wrapped this into a function so that we poststratify per model. Here I also calculate absolute errors for each postratification model prediction.\n\n# Predict over binary outcome mode\nps_health &lt;-  function(model, model_spec, ps) {\n  p &lt;- model |&gt; \n  add_epred_draws(newdata = ps, ndraws=500, allow_new_levels = TRUE) |&gt;\n  rename(pred_good_health = .epred) |&gt;\n  mutate(pred_good_health_prop = pred_good_health * prop) |&gt; \n  ungroup() |&gt; \n  summarise(pred_good_health = sum(pred_good_health_prop),\n            .by = c(zone_id, .draw)) |&gt; \n  summarise(\n    mean = mean(pred_good_health),\n    lower = quantile(pred_good_health, 0.025),\n    upper = quantile(pred_good_health, 0.975),\n    .by = zone_id\n  ) |&gt; \n  left_join(health |&gt; mutate(obs_good = good+verygood) |&gt; select(zone_id, obs_good)) |&gt; \n  left_join(zone_totals) |&gt; \n  mutate(\n    exp_good=mean*hhd, \n    exp_notgood=hhd-exp_good,\n    obs_notgood=hhd-obs_good,\n    sum_tae=abs(exp_good-obs_good) + abs(exp_notgood-obs_notgood),\n    prop_tae=sum_tae/hhd\n    ) |&gt; \n  pivot_longer(cols=c(exp_good, exp_notgood, obs_good, obs_notgood), values_to=\"count\", names_to=\"type\") |&gt; \n  separate_wider_delim(col=type, names=c(\"type\", \"health\"), delim=\"_\") |&gt; \n  pivot_wider(names_from = type, values_from = count) |&gt; \n  add_column(mrp=model_spec)\n \n  return(p)\n}\n\nI then post-stratify over each model, within a purrr::map().\n\nmrp_results &lt;- map2_df(\n  .x=list(model1, model), \n  .y=c(\"model1\", \"model7\"),\n  ~ps_health(.x, .y, ps2 |&gt; mutate(age2=age))\n)\n\nResults from the FMF (spatial microsimulation model) are stored in a separate .csv file. After loading into the session, I refactor this data frame so that it can be joined, with bind_rows, on the MRP results.\nSo bind_rows() on MRP and FMF results.\n\nresults &lt;- bind_rows(\n  fmf_results |&gt; filter(spec %in% c(\"sex_age\", \"educ_imd_region\")) |&gt; \n    mutate(spec=if_else(spec==\"sex_age\", \"model1\", \"model7\"), mean=0, lower=0, upper=0) , \n  mrp_results |&gt; filter(!is.na(hhd)) |&gt;  rename(spec=mrp) |&gt; \n    mutate(type=\"mrp\")  #|&gt; select(-c(mean, lower, upper))\n) |&gt; \nfilter(spec!=\"Census\", spec!=\"RM42_46_120_SxA_qimd19\") |&gt; \n  filter(!is.na(hhd)) |&gt; \n  mutate(\n    perc_obs=obs/hhd, perc_exp=exp/hhd, perc_tae=abs(perc_obs-perc_exp),\n    resid=(obs-exp)/sqrt(exp)\n  ) \n\nAnother external table is that describing Information Entropy scores, per area, on the outcome estimates derived from the spatial microsimulation. This describes how much our models need to draw from a diminishing set of survey respondents, as we constrain on more variables."
  },
  {
    "objectID": "demo/index.html#example-sae-evaluation",
    "href": "demo/index.html#example-sae-evaluation",
    "title": "Experiments with MRP for small area estimation in GIScience",
    "section": "Example SAE evaluation",
    "text": "Example SAE evaluation\nStaging data for plots.\n\nplot_data &lt;- results |&gt; filter(health==\"good\") |&gt; \n mutate(resid=(obs-exp)/sqrt(exp), perc_obs=obs/hhd, perc_exp=exp/hhd) |&gt; \n  group_by(type, spec) |&gt; \n  mutate(\n    resid_est=mean(resid), resid_var=sd(resid),\n    obs_est=mean(obs), obs_var=sd(obs),\n    exp_est=mean(exp), exp_var=sd(exp)\n  ) |&gt; ungroup() |&gt; \n  select(zone_id, type, spec, resid_val=resid, resid_est, resid_var, \n         obs_val=obs, exp_val=exp, obs_est, obs_var, exp_est, exp_var, perc_obs, perc_exp, sum_tae, prop_tae, perc_tae) |&gt; \n  pivot_longer(-c(zone_id, type, spec, sum_tae, prop_tae, perc_tae), names_to=\"stat\") |&gt; \n  separate_wider_delim(stat, delim=\"_\", names=c(\"stat\", \"stat_type\")) |&gt; \n  pivot_wider(names_from=stat_type, values_from=value) |&gt; \n  mutate(lower=est-2*(var/sqrt(6842)), upper=est+2*(var/sqrt(6842))) |&gt; \n  mutate(\n    spec=factor(spec, levels=c(\"model1\", \"model7\"))\n  ) \n\nFor plots evaluating model performance, I generate several helper functions. These are quite specialised to the HSE case, so I need to return to these to generalise when doing comparisons on other model case studies.\n\n\nCode\n# Map residuals\nmap_resids &lt;- function(dat, censor) {\n  \n  # Spatial extent of geog.\n  bbox &lt;- st_bbox(dat)\n  map_width &lt;- bbox$xmax-bbox$xmin\n  map_height &lt;- bbox$ymax-bbox$ymin\n  \n  # Draw map\n  map &lt;- dat |&gt; \n    mutate(resid=pmin(resid,censor)) |&gt; \n  ggplot() +\n  geom_sf(data=. %&gt;% summarise(), fill=\"transparent\", linewidth=.45, colour=\"#525252\") +\n  geom_sf(aes(fill=resid), linewidth=0) +\n  geom_sf(data=. %&gt;% group_by(region) %&gt;% summarise(), fill=\"transparent\", linewidth=.25, colour=\"#ffffff\") +\n  coord_sf(xlim = c(unname(bbox$xmin)+.2*map_width, unname(bbox$xmax)+.5*map_width)) +\n  scale_fill_distiller(palette=\"RdBu\", limits=c(-censor,censor), direction=1)   +\n  # Annotate model performance.\n    geom_text(data = . %&gt;% summarise(),\n    aes(x=bbox$xmax+.2*map_width, y=bbox$ymax-.2*map_height, label=\"MAE\"), hjust=\"right\",vjust=\"top\", size=6.5, colour=\"#525252\",\n  ) +\n  geom_text(data = . %&gt;% summarise(mae = mean(sum_tae),perc_mae= mean(perc_tae)),\n    aes(x=bbox$xmax+.2*map_width, y=bbox$ymax-.28*map_height, label=paste0(\"# \", round(mae,0))), hjust=\"right\",vjust=\"top\", size=6.5, colour=\"#525252\",\n  ) +\n  geom_text(data = . %&gt;% summarise(mae = mean(sum_tae),perc_mae= mean(perc_tae)),\n    aes(x=bbox$xmax+.2*map_width, y=bbox$ymax-.36*map_height, label=paste0(\"% \",round(perc_mae*100,1))), hjust=\"right\",vjust=\"top\", size=6.5, colour=\"#525252\",\n  ) +\n  # Strip out chart assembly.\n  guides(fill=\"none\") +\n  theme(\n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA), \n    axis.text.x = element_blank(), axis.line = element_blank(), \n    axis.text.y = element_blank(), axis.title.x = element_blank(), \n    axis.title.y = element_blank(), \n    plot.margin = margin(0, 0, 0, 0, \"pt\")\n  )\n  \n  bars &lt;- dat |&gt; st_drop_geometry() |&gt;  \n     mutate(resid=pmin(resid,censor)) |&gt;\n    ggplot(aes(x=resid)) +\n    geom_histogram(aes(fill = after_stat(x)), linewidth=0) +\n    geom_step(stat = \"bin\", colour = \"#525252\", linewidth = .15, \n           position = position_nudge(x=-.55)) +\n  \n  # Fill with the same palette as the map\n  scale_fill_distiller(palette=\"RdBu\", limits=c(-censor,censor), direction=1, guide=\"none\")   +\n  scale_x_continuous(limits = c(-censor, censor), expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  labs(x = \"\") +\n  theme(\n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA),\n      plot.margin = margin(0, 0, 0, 0, \"pt\"),\n    axis.text.y=element_blank(), axis.title.y=element_blank(), \n    axis.line = element_blank(), axis.text.x = element_blank()\n  )\n  \n  return(map + inset_element(bars,.65,-.1,1,.55))\n}\n# Map Entropy / Prediction intervals\nmap_intervals &lt;- function(dat, max_scaled) {\n  \n  # Spatial extent of geog.\n  bbox &lt;- st_bbox(dat)\n  map_width &lt;- bbox$xmax-bbox$xmin\n  map_height &lt;- bbox$ymax-bbox$ymin\n  \n  map &lt;- dat |&gt; \n    mutate(interval = interval/max_scaled) |&gt; \n  ggplot() +\n  geom_sf(data=. %&gt;% summarise(), fill=\"transparent\", linewidth=.4, colour=\"#525252\") +\n  geom_sf(aes(fill=interval), linewidth=0) +\n  geom_sf(data= . %&gt;% group_by(region) %&gt;% summarise(), fill=\"transparent\", linewidth=.25, colour=\"#ffffff\") +\n    coord_sf(xlim = c(unname(bbox$xmin)+.1*map_width, unname(bbox$xmax)+.5*map_width)) +\n    scale_fill_distiller(palette=\"Blues\", limits=c(0,1), direction=1, breaks = c(0.1,.9)) +\n    guides(fill=\"none\") +\n    theme(\n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA), \n    axis.text.x = element_blank(), axis.line = element_blank(), \n    axis.text.y = element_blank(), axis.title.x = element_blank(), \n    axis.title.y = element_blank(), \n    plot.margin = margin(0, 0, 0, 0, \"pt\")\n  )\n  \n  bars &lt;- dat |&gt; st_drop_geometry() |&gt;  \n    mutate(interval = interval/max_scaled) |&gt;\n    ggplot(aes(x=interval)) +\n    geom_histogram(aes(fill = after_stat(x)), linewidth=0) +\n    geom_step(stat = \"bin\", colour = \"#525252\", linewidth = .1, \n           position = position_nudge(x=-.015)) +\n  \n  # Fill with the same palette as the map\n  scale_fill_distiller(palette=\"Blues\", limits=c(0,1), direction=1, breaks = c(0.1,.9), guide=\"none\") +\n  scale_x_continuous(limits = c(0, 1), expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  labs(x = \"\") +\n  theme(\n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA),\n      plot.margin = margin(0, 0, 0, 0, \"pt\"),\n    axis.text.y=element_blank(), axis.title.y=element_blank(), \n    axis.line = element_blank(), axis.text.x = element_blank()\n  )\n  \n  return(map + inset_element(bars,.65,-.1,1,.55))\n  \n}\n# Map shannon\nmap_shannon &lt;- function(dat, max_scaled) {\n  \n  # Spatial extent of geog.\n  bbox &lt;- st_bbox(dat)\n  map_width &lt;- bbox$xmax-bbox$xmin\n  map_height &lt;- bbox$ymax-bbox$ymin\n  \n  map &lt;- dat |&gt; \n    mutate(entropy = shannon/max_scaled) |&gt; \n  ggplot() +\n  geom_sf(data=. %&gt;% summarise(), fill=\"transparent\", linewidth=.4, colour=\"#525252\") +\n  geom_sf(aes(fill=entropy), linewidth=0) +\n  geom_sf(data= . %&gt;% group_by(region) %&gt;% summarise(), fill=\"transparent\", linewidth=.25, colour=\"#ffffff\") +\n    coord_sf(xlim = c(unname(bbox$xmin)+.1*map_width, unname(bbox$xmax)+.5*map_width)) +\n    scale_fill_distiller(palette=\"Blues\", limits=c(0,1), direction=-1, breaks = c(0.1,.9)) +\n    guides(fill=\"none\") +\n    theme(\n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA), \n    axis.text.x = element_blank(), axis.line = element_blank(), \n    axis.text.y = element_blank(), axis.title.x = element_blank(), \n    axis.title.y = element_blank(), \n    plot.margin = margin(0, 0, 0, 0, \"pt\")\n  )\n  \n  bars &lt;- dat |&gt; st_drop_geometry() |&gt;  \n    mutate(entropy = shannon/max_scaled) |&gt;\n    ggplot(aes(x=entropy)) +\n    geom_histogram(aes(fill = after_stat(x)), linewidth=0) +\n    geom_step(stat = \"bin\", colour = \"#525252\", linewidth = .1, \n           position = position_nudge(x=-.015)) +\n  \n  # Fill with the same palette as the map\n  scale_fill_distiller(palette=\"Blues\", limits=c(0,1), direction=-1, breaks = c(0.1,.9), guide=\"none\") +\n  scale_x_continuous(limits = c(0, 1), expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  labs(x = \"\") +\n  theme(\n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA),\n      plot.margin = margin(0, 0, 0, 0, \"pt\"),\n    axis.text.y=element_blank(), axis.title.y=element_blank(), \n    axis.line = element_blank(), axis.text.x = element_blank()\n  )\n  \n  return(map + inset_element(bars,.65,-.1,1,.55))\n  \n}\n# Model labs\nmodel_labs&lt;- function(spec) {\n p &lt;-  ggplot() +\n    annotate(\"richtext\", x=0,y=.5, label=spec, hjust=\"left\",vjust=\"middle\", label.colour = NA, size=9, colour=\"#525252\", fill=\"transparent\") +\n   scale_x_continuous(limits=c(0,1), expand=c(0, 0)) +\n   scale_y_continuous(limits=c(0,1), expand=c(0, 0)) +\n  theme_void() +\n     theme(\n    plot.margin = unit(c(0, 0, 0, 0), \"pt\"),        \n    panel.background = element_rect(fill = NA, color = NA),  \n    plot.background = element_rect(fill = NA, color = NA)   \n  )\nreturn(p)\n}\n\n\nI then iterate over each model and generate plot summaries: maps of residuals, of prediction intervals (MRP) and Information Emntropy (SPM).\n\n\nCode\nsubsets_order &lt;- c(\"model1\", \"model7\")\nsubsets &lt;- tibble(type=c(rep(\"mrp\",2), rep(\"fmf\",2)), spec=rep(subsets_order,2))\np_resids &lt;- pmap(subsets,  \n                 ~map_resids(\n                   dat=results |&gt;\n                     filter(health==\"good\", type==..1, spec==..2) |&gt; \n                     left_join(msoa_boundaries, by=c(\"zone_id\"=\"MSOA21CD\")) |&gt; st_as_sf() |&gt; \n                     left_join(geog |&gt; select(zone_id, region)), \n                   censor=20)\n)\nmax_scaled &lt;- results |&gt; filter(type == \"mrp\") |&gt; mutate(interval=upper-lower) |&gt;  \n  group_by(spec) |&gt; \n  summarise(max=max(interval))\np_intervals &lt;- pmap(subsets |&gt; filter(type == \"mrp\"), \n                 ~map_intervals(\n                   dat=results |&gt;\n                     filter(type==..1, spec==..2) |&gt; \n                     left_join(msoa_boundaries, by=c(\"zone_id\"=\"MSOA21CD\")) |&gt; st_as_sf() |&gt; \n                     left_join(geog |&gt; select(zone_id, region)) |&gt; \n                     mutate(interval=upper-lower),\n                   max_scaled |&gt; filter(spec==..2) |&gt; pull(max)\n                   )\n                 )\n\nmax_scaled &lt;- shannon |&gt; group_by(spec) |&gt;  summarise(max=max(shannon)) \np_shannon &lt;- pmap(subsets |&gt; filter(type == \"fmf\"), \n                 ~map_shannon(\n                   dat=shannon |&gt;\n                     filter(type==..1, spec==..2) |&gt; \n                     left_join(msoa_boundaries, by=c(\"zone_id\"=\"MSOA21CD\")) |&gt; st_as_sf() |&gt; \n                     left_join(geog |&gt; select(zone_id, region)),\n                    max_scaled |&gt; filter(spec==..2) |&gt; pull(max)\n                   )\n                 )\np_model_labs &lt;- map(subsets |&gt; filter(type==\"mrp\") |&gt;  select(spec) |&gt; pull(), ~model_labs(.x))\n\n# Generate patchwork plots for each model summary type\ncomposite_labs &lt;- (p_model_labs[[1]] / p_model_labs[[2]])\ncomposite_resids_mrp &lt;- (p_resids[[1]] / p_resids[[2]])\ncomposite_resids_spm &lt;- (p_resids[[3]] / p_resids[[4]])\ncomposite_shannon &lt;- (p_shannon[[1]] / p_shannon[[2]])\ncomposite_intervals &lt;- (p_intervals[[1]] / p_intervals[[2]])\n\n# View composition of each of these summaries.\ncomp_plot &lt;- (composite_labs | composite_resids_mrp | composite_intervals |  composite_resids_spm | composite_shannon) + plot_layout(widths=c(.25,1,.9, 1,.9))\ncomp_plot_gg &lt;- patchworkGrob(comp_plot)\n\n# Annotate composed plots.\np &lt;- ggplot() +\n  scale_y_continuous(limits=c(-.1,1.05), expand=c(0, 0)) +\n  scale_x_continuous(limits=c(0,1.05), expand=c(0, 0)) +\n  annotation_custom(comp_plot_gg, xmin = 0, ymin = 0, xmax = 1, ymax = 1) +\n\n  theme(\n    axis.title.x=element_blank(), axis.title.y=element_blank(),\n    axis.text = element_blank(), axis.line = element_blank(),\n    plot.margin = unit(c(0, 0, 0, 0), \"pt\"),       \n    panel.background = element_rect(fill = \"#ffffff\", color = NA),  \n    plot.background = element_rect(fill = \"#ffffff\", color = NA)    \n  ) +\n  \n  \n  # Model titles\n    annotate(\"richtext\", x=.25,y=1.05, hjust=\"left\",vjust=\"top\", label.colour = NA, size=12, colour=\"#525252\", fill=\"transparent\",\n           label=\"MRP\" ) +\n    annotate(\"richtext\", x=.73,y=1.05, hjust=\"left\",vjust=\"top\", label.colour = NA, size=12, colour=\"#525252\", fill=\"transparent\",\n           label=\"SPM\" ) +\n\n  \n  # Legends: fiddly use of annotate, as &lt;br&gt; unexpected in ggtext.\n  annotate(\"segment\", x=0,y=0, xend=1.05, yend=0, colour=\"#525252\", linewidth=.1) +\n\n    annotate(\"richtext\", x=.08,y=0, hjust=\"left\",vjust=\"top\", label.colour = NA, \n  size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"Residuals: obs-model / sqrt(model)\" ) +\n    annotate(\"richtext\", x=.1,y=-.03, hjust=\"left\",vjust=\"top\", label.colour = NA, size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"&lt;span style='color: #b2182b'&gt;**underestimate**&lt;/span&gt; |\n           &lt;span style='color: #2166ac;'&gt;**overestimate**&lt;/span&gt;\" ) +\n  \n   annotate(\"richtext\", x=.31,y=.0, hjust=\"left\",vjust=\"top\", label.colour = NA, size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"Prediction intervals\") +\n  annotate(\"richtext\", x=.31,y=-.03, hjust=\"left\",vjust=\"top\", label.colour = NA, size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"&lt;span style='color: #9ecae1;'&gt;**narrower**&lt;/span&gt; | &lt;span style='color: #084594;'&gt;**wider**&lt;/span&gt;\") +\n           \n  annotate(\"richtext\", x=.8,y=.0, hjust=\"left\",vjust=\"top\", label.colour = NA, size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"Entropy\") +\n    annotate(\"richtext\", x=.8, y=-.03, hjust=\"left\",vjust=\"top\", \n             label.colour = NA, size=6.5, colour=\"#525252\", \n             fill=\"transparent\",\n             label = \"&lt;span style='color: #9ecae1;'&gt;**high diversity**&lt;/span&gt; | &lt;span style='color: #084594;'&gt;**low diversity**&lt;/span&gt;&lt;br&gt;~ more respondents\") +\n      \n  annotate(\"richtext\", x=.55,y=0, hjust=\"left\", vjust=\"top\", label.colour = NA, size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"Mean abs error:\" ) +\n    annotate(\"richtext\", x=.55,y=-.03, hjust=\"left\", vjust=\"top\", label.colour = NA, size=6.5, colour=\"#525252\", fill=\"transparent\",\n           label=\"&lt;span&gt;# good & bad | % good&lt;/span&gt;\" ) +\n  \n  annotate(\"text\", x=.67,y=.065, hjust=\"left\",vjust=\"top\",  size=6.5, colour=\"#525252\", \n           label=\"Extreme estimates\") +\n  annotate(\"text\", x=.67,y=.035, hjust=\"left\",vjust=\"top\",  size=6.5, colour=\"#525252\", \n           label=\"are censored\") +\n  annotate(\"curve\", x=.72, xend=.76, y=.075, yend=.12, linewidth=.2, colour=\"#525252\", arrow = arrow(length = unit(0.006, \"npc\")), curvature = -0.4)  \n\nggsave(here(\"..\", \"figs\", \"model_summary.png\"), p, width=8, height=4, dpi=300)\n\n\n\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SMART-MRP",
    "section": "",
    "text": "This website contains details and readings on our proposal, SMART-MRP: Simulating place-based outcomes from behavioural datasets.\nThe proposal is motivated by some existing (and ongoing) work on inserting MRP into GIScience: Our 2025 GISRUK abstract, Should we use Multilevel Regression and Post-stratification when simulating area-level population outcomes?, and associated talk.\n\n\nRoger Beecham"
  }
]